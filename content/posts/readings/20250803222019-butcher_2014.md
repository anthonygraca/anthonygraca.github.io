+++
title = "butcher-2014"
author = ["Anthony Graca"]
date = 2025-08-03T22:20:00-07:00
tags = ["Bibliography"]
draft = false
+++

## Citation {#citation}

```tex
@book{butcher2014,
  title={Seven Concurrency Models in Seven Weeks: When Threads Unravel},
  author={Butcher, Paul},
  year={2014},
  publisher={The Pragmatic Bookshelf}
}
```


## Summary. What are the statements being made? {#summary-dot-what-are-the-statements-being-made}


## 1. Introduction {#1-dot-introduction}


### Concurrent or Parallel? {#concurrent-or-parallel}

-   Concurrent and parallel refer to two related but different things
-   "A _concurrent_ program has multiple logical _threads of control_. These threads may or may not run in parallel (Butcher 2014, 1)."
    -   Concurrency is an aspect of the problem domain, meaning your algorithm needs to handle simultaneous events
    -   "Concurrency is about dealing with lots of things at once - rob pike"
-   "A _parallel_ program potentially runs more quickly than a sequential program by executing different parts of the computation simultaneously in parallel. It may or may not have more than one logical thread of control"
    -   Parallelism is an aspect of the solution domain, meaning you want to make your program faster
    -   "Parallelism is about doing lots of things at once - rob pike"
-   Traditional threads and locks don't provide any direct support for parallelism.
    -   In order to exploit multiple cores with threads and locks, you need to create a concurrent program and then run it on parallel hardware.
    -   This is problematic because concurrent programs are nondeterministic but parallel programs are usually not


### Parallel Architecture {#parallel-architecture}

-   There are multiple levels of parallelism
    -   Moving from 8-bits to 32-bits is a form of _Bit-Level Parallelism_. Adding two 32-bit numbers in a 8-bit architecture would take multiple steps. Doing this operation in a 32-bit system is done in a single step
    -   CPU architectures use pipeling, out-of-order execution, and speculative execution to obtain instruction _instruction-level parallelism_
    -   Data-parallelism is achieved by applying the same operations to a large amount of data in parallel. Imagine increasing the brightness of an image where each pixel easily handled by a GPU
-   What we are interested in is **Task-Level Parallelism**
    -   There are two models of multiprocessor architectures:
        1.  Shared-memory :: where each processor can access any memory location and interprocess communication is done through memory
        2.  Distributed memory :: where each processor has its own local memory and IPC is done via the network.


### Concurrency: Beyond Multiple Cores {#concurrency-beyond-multiple-cores}

-   Concurrency is key to software being responsive, fault tolerant, efficient, and simple


#### Responsive {#responsive}

-   the world is concurrent so software should be concurrent to interact with it properly
-   Examples:
    1.  a mobile phone can play music, talk to the network, and pay attention to touch gestures all at the same time
    2.  an IDE checks for syntax in the background while code is being typed
    3.  A flight system simultaneously monitors sensors, displays information to the pilot, obeys commands, and moves control surfaces
        -   i think of [douglass-2003]({{< relref "20230330175118-douglass_2003.md" >}})
-   Concurrency is key to _responsive_ systems.
    -   Doing things in the background avoids forcing users to wait and stare at a loading screen.


#### Fault Tolerant {#fault-tolerant}

-   Distributed systems are fault tolerant. Shutdown on one data center doesn't halt the entire system
-   Concurrency also enables _fault detection_.
    -   A task that fails can notify a separate task to perform remedial action.
    -   Sequential software can never be as resilient as concurrent software.


#### Simple {#simple}

-   Threading bugs are difficult to diagnose.
-   Concurrent solutions can be simpler and clearer than its sequential equivalent
    -   Translating a concurrent real-world problem to its sequential solution hides detail and requires more work


### The Seven Models {#the-seven-models}

1.  Threads and locks
2.  Functional programming
    -   eliminates mutable state so functional programs are intrinsically thread-safe
3.  The Clojure Way - Separating identity and state
4.  Actors
    -   Concurrent programming model with strong support for fault tolerance and resilence
5.  Communicating Sequential Processes
    -   Emphasizes channels for communication
6.  Data Parallelism - Using GPUs
7.  The Lambda Architecture
    -   Big data with MapReduce and stream processing to handle terabytes of data


## 2. Threads and Locks {#2-dot-threads-and-locks}

-   "Threads-and-locks programming is like a Ford Model T. It will get you from point A to point B, but it is primitive, difficult to drive, and both unreliable and dangerous compared to newer technology (Butcher 2014, 9).
-   p 9


### The Simplest Thing that Could Possibly Work {#the-simplest-thing-that-could-possibly-work}


### Day 1: Mutual Exclusion and Memory Models {#day-1-mutual-exclusion-and-memory-models}


### Day 2: Beyond Intrinsic Locks {#day-2-beyond-intrinsic-locks}


### Day 3: On the Shoulders of Giants {#day-3-on-the-shoulders-of-giants}


## 3. Functional Programming {#3-dot-functional-programming}

-   p 49


### If it Hurts, Stop Doing It {#if-it-hurts-stop-doing-it}


### Day 1: Programming Without Mutable State {#day-1-programming-without-mutable-state}


### Day 2: Functional Parallelism {#day-2-functional-parallelism}


### Day 3: Functional Concurrency {#day-3-functional-concurrency}


## 4. The Clojure Way - Separating Identity from State {#4-dot-the-clojure-way-separating-identity-from-state}

-   p 85


### The Best of Both Worlds {#the-best-of-both-worlds}


### Day 1: Atoms and Persistent Data Structures {#day-1-atoms-and-persistent-data-structures}


### Day 2: Agents and Software Transactional memory {#day-2-agents-and-software-transactional-memory}


### Day 3: In Depth {#day-3-in-depth}


## 5. Actors {#5-dot-actors}

-   p 115


### More Object-Oriented than Objects {#more-object-oriented-than-objects}


### Day 1: Messages and Mailboxes {#day-1-messages-and-mailboxes}


### Day 2: Error Handling and Resilience {#day-2-error-handling-and-resilience}


### Day 3: Distribution {#day-3-distribution}


## 6. Communicating Sequential Processes {#6-dot-communicating-sequential-processes}

-   p 153


### Communication Is Everything {#communication-is-everything}


### Day 1: Channels and Go Blocks {#day-1-channels-and-go-blocks}


### Day 2: Multiple Channels and IO {#day-2-multiple-channels-and-io}


### Day 3: Client-Side CSP {#day-3-client-side-csp}


## 7. Data Parallelism {#7-dot-data-parallelism}

-   p 189


### The Supercomputer Hidden in Your Laptop {#the-supercomputer-hidden-in-your-laptop}


### Day 1: GPGPU Programming {#day-1-gpgpu-programming}


### Day 2: multiple Dimensions and Work-Groups {#day-2-multiple-dimensions-and-work-groups}


### Day 3: OpenCL and OpenGL - Keeping it on the GPU {#day-3-opencl-and-opengl-keeping-it-on-the-gpu}


## 8. The Lambda Architecture {#8-dot-the-lambda-architecture}

-   p 223


### Parallelism Enables Big Data {#parallelism-enables-big-data}


### Day 1: MapReduce {#day-1-mapreduce}


### Day 2: The Batch Layer {#day-2-the-batch-layer}


### Day 3: The Speed Layer {#day-3-the-speed-layer}


## 9. Wrapping Up {#9-dot-wrapping-up}

-   p 263


## Next {#next}

-   [tate-2010]({{< relref "20250803223046-tate_2010.md" >}})
-   [herlihy-2020]({{< relref "20250804113206-herlihy_2020.md" >}})

-   [douglass-2003]({{< relref "20230330175118-douglass_2003.md" >}})
-   [williams-2019]({{< relref "20230112115043-williams_2019.md" >}})

-   [dean-ghemawat-2008]({{< relref "article/20240921135242-dean_ghemawat_2008.md" >}})
